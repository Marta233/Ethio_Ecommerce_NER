{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":9509572,"sourceType":"datasetVersion","datasetId":5788325}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef load_conll_format(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = []\n        sentence = []\n        labels = []\n\n        for line in f:\n            line = line.strip()\n            if line == \"\":\n                if sentence:  # Only append if the sentence list is not empty\n                    data.append((sentence, labels))\n                    sentence = []\n                    labels = []\n            else:\n                parts = line.split()\n                if len(parts) == 2:  # Ensure there are exactly two parts\n                    token, label = parts\n                    sentence.append(token)\n                    labels.append(label)\n                else:\n                    print(f\"Skipping line: {line}\")  # Optional: print to debug which lines are problematic\n\n        if sentence:  # Append the last sentence if the file doesn't end with a newline\n            data.append((sentence, labels))\n\n    return pd.DataFrame(data, columns=['tokens', 'labels'])\n\ndf = load_conll_format(\"/kaggle/input/datset11/dataset.txt\")","metadata":{"id":"TUo0s-HgYtw1","execution":{"iopub.status.busy":"2024-09-29T14:47:45.807958Z","iopub.execute_input":"2024-09-29T14:47:45.808396Z","iopub.status.idle":"2024-09-29T14:47:45.831548Z","shell.execute_reply.started":"2024-09-29T14:47:45.808355Z","shell.execute_reply":"2024-09-29T14:47:45.830370Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Skipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ግራንድI-LOC\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\n","output_type":"stream"}]},{"cell_type":"code","source":"df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OvJ5qb7pYxN6","outputId":"3019854e-c3c3-473c-9d4b-00e6bf99bfb0","execution":{"iopub.status.busy":"2024-09-29T14:48:01.893505Z","iopub.execute_input":"2024-09-29T14:48:01.893995Z","iopub.status.idle":"2024-09-29T14:48:01.929231Z","shell.execute_reply.started":"2024-09-29T14:48:01.893950Z","shell.execute_reply":"2024-09-29T14:48:01.928260Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                               tokens  \\\n0   [2, ሊትር, ፔርሙስ, ለቤት, ለቢሮ, ለሆቴል, አገልግሎት, መዋል, የሚ...   \n1   [ቢላ, የስጋ, የአጥንት, 1000, ብር, ለማዘዝ, ውስን, ፍሬ, ነው, ...   \n2   [ዋስትና, ቅናሽ, አድራሻ, ቁጥር, 1, መገናኛ, ዘፍመሽ, ግራንድ, ሞል...   \n3   [2, ️, አንድ, ብሩሽ, እና, አንድ, ስፓቹላ, ዋጋ, 300, ብር, ለ...   \n4   [2, ️, አንድ, ብሩሽ, እና, አንድ, ስፓቹላ, ዋጋ, 300, ብር, ው...   \n..                                                ...   \n56  [ዋጋ, 1000, ብር, ለማዘዝ, ውስን, ፍሬ, ነው, የቀረው, ️ጥራት, ...   \n57  [ለቤቶ, ለስጦታ, በጥራት, በቅናሽ, ውስን, ፍሬ, የቀሩ, ዕቃወች, አሁ...   \n58  [ለልጆ, ለስጦታ, በጥራት, በቅናሽ, ውስን, ፍሬ, የቀሩ, ዕቃወች, አሁ...   \n59  [4, 1, 400, 10000, ብር, ከ, ነፃ, ማድረስ, ጋር, ለማዘዝ, ...   \n60  [4, 1, 400, 10000, ብር, ከ, ነፃ, ማድረስ, ጋር, ለማዘዝ, ...   \n\n                                               labels  \n0   [O, O, O, O, O, O, O, O, O, O, B-PRODUCT, O, O...  \n1   [B-PRODUCT, O, O, B-PRICE, I-PRICE, O, O, O, O...  \n2   [O, O, O, O, O, B-LOC, I-LOC, I-LOC, I-LOC, I-...  \n3   [O, O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE,...  \n4   [O, O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE,...  \n..                                                ...  \n56  [B-PRICE, I-PRICE, I-PRICE, O, O, O, O, O, O, ...  \n57  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n58  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n59  [O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...  \n60  [O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...  \n\n[61 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokens</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[2, ሊትር, ፔርሙስ, ለቤት, ለቢሮ, ለሆቴል, አገልግሎት, መዋል, የሚ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, B-PRODUCT, O, O...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[ቢላ, የስጋ, የአጥንት, 1000, ብር, ለማዘዝ, ውስን, ፍሬ, ነው, ...</td>\n      <td>[B-PRODUCT, O, O, B-PRICE, I-PRICE, O, O, O, O...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[ዋስትና, ቅናሽ, አድራሻ, ቁጥር, 1, መገናኛ, ዘፍመሽ, ግራንድ, ሞል...</td>\n      <td>[O, O, O, O, O, B-LOC, I-LOC, I-LOC, I-LOC, I-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[2, ️, አንድ, ብሩሽ, እና, አንድ, ስፓቹላ, ዋጋ, 300, ብር, ለ...</td>\n      <td>[O, O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[2, ️, አንድ, ብሩሽ, እና, አንድ, ስፓቹላ, ዋጋ, 300, ብር, ው...</td>\n      <td>[O, O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>[ዋጋ, 1000, ብር, ለማዘዝ, ውስን, ፍሬ, ነው, የቀረው, ️ጥራት, ...</td>\n      <td>[B-PRICE, I-PRICE, I-PRICE, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>[ለቤቶ, ለስጦታ, በጥራት, በቅናሽ, ውስን, ፍሬ, የቀሩ, ዕቃወች, አሁ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>[ለልጆ, ለስጦታ, በጥራት, በቅናሽ, ውስን, ፍሬ, የቀሩ, ዕቃወች, አሁ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>[4, 1, 400, 10000, ብር, ከ, ነፃ, ማድረስ, ጋር, ለማዘዝ, ...</td>\n      <td>[O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>[4, 1, 400, 10000, ብር, ከ, ነፃ, ማድረስ, ጋር, ለማዘዝ, ...</td>\n      <td>[O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>61 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = 'bert-base-multilingual-cased'  # or \"distilbert-base-multilingual-cased\", \"bert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndef tokenize_and_align_labels(dataframe):\n    # Tokenize the inputs\n    tokenized_inputs = tokenizer(\n        list(dataframe['tokens']),\n        truncation=True,\n        is_split_into_words=True,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n\n    labels = []\n    for i, label in enumerate(dataframe['labels']):\n        # Get word IDs for the current batch\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        # Initialize label_ids with -100 for padding\n        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])\n\n        for word_index in range(len(word_ids)):\n            if word_ids[word_index] is not None:\n                # Get the label for the current token\n                current_label = label[word_ids[word_index]]\n                if current_label in label_to_id:\n                    label_ids[word_index] = label_to_id[current_label]  # Assign the corresponding label id\n                else:\n                    print(f\"Warning: Label '{current_label}' not found in label_to_id. Using -100.\")\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Define label mappings\nlabel_to_id = {\n    \"O\": 0,\n    \"B-PRICE\": 1,\n    \"I-PRICE\": 2,\n    \"B-LOC\": 3,\n    \"I-LOC\": 4,\n    \"B-PRODUCT\": 5,\n    \"I-PRODUCT\": 6,\n    # Add other labels as needed\n}\n\n# Tokenize the data\ntokenized_data = tokenize_and_align_labels(df)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsl4T35HNop3","outputId":"6188c1d9-9b2f-4982-d323-445cdb5084ef","execution":{"iopub.status.busy":"2024-09-29T15:14:28.111883Z","iopub.execute_input":"2024-09-29T15:14:28.112375Z","iopub.status.idle":"2024-09-29T15:14:28.765247Z","shell.execute_reply.started":"2024-09-29T15:14:28.112330Z","shell.execute_reply":"2024-09-29T15:14:28.763987Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"try:\n    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_to_id))\n    print(f\"Successfully loaded model: {model_name}\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:17:01.685304Z","iopub.execute_input":"2024-09-29T15:17:01.685812Z","iopub.status.idle":"2024-09-29T15:17:02.111511Z","shell.execute_reply.started":"2024-09-29T15:17:01.685764Z","shell.execute_reply":"2024-09-29T15:17:02.110175Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Successfully loaded model: bert-base-multilingual-cased\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelForTokenClassification\n\n# Load model\nmodel = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_to_id))\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n    logging_steps=10,  # Log after every 10 steps\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",  # Directory to save logs\n    logging_first_step=True,  # Log the first step as well\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGlwqFLENs4n","outputId":"6f2a2427-fa97-43a5-a99f-a9d6b7e6209a","execution":{"iopub.status.busy":"2024-09-29T15:17:07.287186Z","iopub.execute_input":"2024-09-29T15:17:07.288061Z","iopub.status.idle":"2024-09-29T15:17:07.722119Z","shell.execute_reply.started":"2024-09-29T15:17:07.288014Z","shell.execute_reply":"2024-09-29T15:17:07.720985Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"df = load_conll_format('/kaggle/input/datset11/dataset.txt')\ntokenized_data = tokenize_and_align_labels(df)\nfrom datasets import Dataset\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_dict({\n    'input_ids': tokenized_data['input_ids'],\n    'attention_mask': tokenized_data['attention_mask'],\n    'labels': tokenized_data['labels']\n})\n\nprint(f\"Number of samples in the dataset: {len(dataset)}\")\n\nif len(dataset) > 1:\n    # Split the dataset for training and validation\n    train_dataset, val_dataset = dataset.train_test_split(test_size=0.2).values()\nelse:\n    # Use the entire dataset for training\n    train_dataset = dataset\n    val_dataset = dataset  # Or create a separate validation dataset if needed\n\n# Set up Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"kNfJ1thxOEU4","outputId":"7cfc57a0-430d-40d4-ceb4-85f7eece7cf8","execution":{"iopub.status.busy":"2024-09-29T15:17:12.149256Z","iopub.execute_input":"2024-09-29T15:17:12.149764Z","iopub.status.idle":"2024-09-29T15:19:07.628469Z","shell.execute_reply.started":"2024-09-29T15:17:12.149687Z","shell.execute_reply":"2024-09-29T15:19:07.627303Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Skipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\nSkipping line: ግራንድI-LOC\nSkipping line: ️\nSkipping line: ️\nSkipping line: ️\nNumber of samples in the dataset: 61\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 01:42, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.973800</td>\n      <td>0.640335</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.973800</td>\n      <td>0.575930</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.973800</td>\n      <td>0.565332</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9, training_loss=0.8299133910073174, metrics={'train_runtime': 114.8245, 'train_samples_per_second': 1.254, 'train_steps_per_second': 0.078, 'total_flos': 9333615433824.0, 'train_loss': 0.8299133910073174, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"eval_result = trainer.evaluate()\nprint(eval_result)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"ymZ4RxpyOLDq","outputId":"7f1cf1ef-9f20-466a-da1f-4ea83b035894","execution":{"iopub.status.busy":"2024-09-29T15:03:44.383650Z","iopub.execute_input":"2024-09-29T15:03:44.384170Z","iopub.status.idle":"2024-09-29T15:03:52.155114Z","shell.execute_reply.started":"2024-09-29T15:03:44.384124Z","shell.execute_reply":"2024-09-29T15:03:52.153931Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 1.445087194442749, 'eval_runtime': 7.7518, 'eval_samples_per_second': 1.677, 'eval_steps_per_second': 0.129, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# Model names for fine-tuning\nmodel_names = {\n    \"xlm-roberta\": \"xlm-roberta-base\",\n    \"distilbert\": \"distilbert-base-multilingual-cased\",\n    \"mbert\": \"bert-base-multilingual-cased\"\n}\n\ntokenizers = {}\nmodels = {}\n\n# Load tokenizers and models\nfor model_key, model_name in model_names.items():\n    tokenizers[model_key] = AutoTokenizer.from_pretrained(model_name)\n    models[model_key] = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_to_id))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":217},"id":"KHOtWLpFOcqS","outputId":"83ae77da-801a-4bf6-88ee-9fbe9dc72021","execution":{"iopub.status.busy":"2024-09-29T15:19:58.015308Z","iopub.execute_input":"2024-09-29T15:19:58.015776Z","iopub.status.idle":"2024-09-29T15:20:02.805820Z","shell.execute_reply.started":"2024-09-29T15:19:58.015732Z","shell.execute_reply":"2024-09-29T15:20:02.804523Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Define function to tokenize and align labels\ndef tokenize_and_align_labels(dataframe, tokenizer):\n    tokenized_inputs = tokenizer(\n        list(dataframe['tokens']),\n        truncation=True,\n        is_split_into_words=True,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n\n    labels = []\n    for i, label in enumerate(dataframe['labels']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])  # Use -100 for padding tokens\n        \n        for word_index in range(len(word_ids)):\n            if word_ids[word_index] is not None:  # Check if it's a valid word token\n                current_label = label[word_ids[word_index]]\n                label_ids[word_index] = label_to_id[current_label]\n        \n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Tokenize data for each model\ntokenized_data = {}\nfor model_key, tokenizer in tokenizers.items():\n    tokenized_data[model_key] = tokenize_and_align_labels(df, tokenizer)\n","metadata":{"id":"yfvLXCVsQ9TC","execution":{"iopub.status.busy":"2024-09-29T15:24:44.704038Z","iopub.execute_input":"2024-09-29T15:24:44.705365Z","iopub.status.idle":"2024-09-29T15:24:44.826861Z","shell.execute_reply.started":"2024-09-29T15:24:44.705277Z","shell.execute_reply":"2024-09-29T15:24:44.825484Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Convert the tokenized data into Hugging Face Datasets for training and validation\ndatasets = {}\nfor model_key, data in tokenized_data.items():\n    dataset = Dataset.from_dict({\n        'input_ids': data['input_ids'],\n        'attention_mask': data['attention_mask'],\n        'labels': data['labels']\n    })\n\n    # Split the dataset into training and validation sets (80/20 split)\n    datasets[model_key] = dataset.train_test_split(test_size=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:25:00.309820Z","iopub.execute_input":"2024-09-29T15:25:00.310344Z","iopub.status.idle":"2024-09-29T15:25:00.403230Z","shell.execute_reply.started":"2024-09-29T15:25:00.310297Z","shell.execute_reply":"2024-09-29T15:25:00.402243Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n)\n\n# Fine-tune each model and store the trainers\ntrainers = {}\nfor model_key, model in models.items():\n    trainers[model_key] = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=datasets[model_key]['train'],\n        eval_dataset=datasets[model_key]['test']\n    )\n\n    # Train the model\n    print(f\"Training model: {model_key}\")\n    trainers[model_key].train()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:25:17.589328Z","iopub.execute_input":"2024-09-29T15:25:17.589791Z","iopub.status.idle":"2024-09-29T15:32:22.300995Z","shell.execute_reply.started":"2024-09-29T15:25:17.589747Z","shell.execute_reply":"2024-09-29T15:32:22.299780Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Training model: xlm-roberta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 03:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.844292</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.591909</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.445087</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training model: distilbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.009381</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.723416</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.665008</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training model: mbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 01:36, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.755505</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.665313</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.637359</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the models on the validation dataset\nresults = {}\nfor model_key, trainer in trainers.items():\n    print(f\"Evaluating model: {model_key}\")\n    eval_result = trainer.evaluate()\n    results[model_key] = eval_result\n\n# Print the results\nfor model_key, result in results.items():\n    print(f\"Results for {model_key}: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:32:44.689333Z","iopub.execute_input":"2024-09-29T15:32:44.689843Z","iopub.status.idle":"2024-09-29T15:32:58.696546Z","shell.execute_reply.started":"2024-09-29T15:32:44.689792Z","shell.execute_reply":"2024-09-29T15:32:58.695152Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Evaluating model: xlm-roberta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 02:15]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluating model: distilbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 02:13]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluating model: mbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 02:13]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Results for xlm-roberta: {'eval_loss': 1.445087194442749, 'eval_runtime': 8.1892, 'eval_samples_per_second': 1.587, 'eval_steps_per_second': 0.122, 'epoch': 3.0}\nResults for distilbert: {'eval_loss': 0.6650081872940063, 'eval_runtime': 2.6282, 'eval_samples_per_second': 4.946, 'eval_steps_per_second': 0.38, 'epoch': 3.0}\nResults for mbert: {'eval_loss': 0.6373586058616638, 'eval_runtime': 3.0899, 'eval_samples_per_second': 4.207, 'eval_steps_per_second': 0.324, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the models on the validation dataset\nresults = {}\nfor model_key, trainer in trainers.items():\n    print(f\"Evaluating model: {model_key}\")\n    eval_result = trainer.evaluate()\n    results[model_key] = eval_result\n\n# Print the evaluation results for each model\nfor model_key, result in results.items():\n    print(f\"Results for {model_key}: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:35:01.845795Z","iopub.execute_input":"2024-09-29T15:35:01.847057Z","iopub.status.idle":"2024-09-29T15:35:11.856413Z","shell.execute_reply.started":"2024-09-29T15:35:01.846997Z","shell.execute_reply":"2024-09-29T15:35:11.855093Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Evaluating model: xlm-roberta\nEvaluating model: distilbert\nEvaluating model: mbert\nResults for xlm-roberta: {'eval_loss': 1.445087194442749, 'eval_runtime': 6.1911, 'eval_samples_per_second': 2.1, 'eval_steps_per_second': 0.162, 'epoch': 3.0}\nResults for distilbert: {'eval_loss': 0.6650081872940063, 'eval_runtime': 1.2881, 'eval_samples_per_second': 10.092, 'eval_steps_per_second': 0.776, 'epoch': 3.0}\nResults for mbert: {'eval_loss': 0.6373586058616638, 'eval_runtime': 2.5019, 'eval_samples_per_second': 5.196, 'eval_steps_per_second': 0.4, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    # Flatten and filter out padding tokens (-100)\n    true_labels = []\n    predicted_labels = []\n    \n    for i in range(len(labels)):\n        true_labels.extend([label for label, pred_label in zip(labels[i], preds[i]) if label != -100])\n        predicted_labels.extend([pred_label for label, pred_label in zip(labels[i], preds[i]) if label != -100])\n    \n    # Compute metrics\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    precision = precision_score(true_labels, predicted_labels, average=\"macro\")\n    recall = recall_score(true_labels, predicted_labels, average=\"macro\")\n    f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:40:45.719948Z","iopub.execute_input":"2024-09-29T15:40:45.720620Z","iopub.status.idle":"2024-09-29T15:40:45.736793Z","shell.execute_reply.started":"2024-09-29T15:40:45.720559Z","shell.execute_reply":"2024-09-29T15:40:45.734934Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"trainers = {}\nfor model_key, model in models.items():\n    trainers[model_key] = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=datasets[model_key]['train'],\n        eval_dataset=datasets[model_key]['test'],\n        compute_metrics=compute_metrics  # Use the updated compute_metrics function\n    )\n\n# Evaluate the models\nresults = {}\nfor model_key, trainer in trainers.items():\n    print(f\"Evaluating model: {model_key}\")\n    eval_result = trainer.evaluate()\n    results[model_key] = eval_result\n\n# Print results\nfor model_key, result in results.items():\n    print(f\"Results for {model_key}: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:40:48.765556Z","iopub.execute_input":"2024-09-29T15:40:48.766039Z","iopub.status.idle":"2024-09-29T15:40:58.641215Z","shell.execute_reply.started":"2024-09-29T15:40:48.765991Z","shell.execute_reply":"2024-09-29T15:40:58.640191Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Evaluating model: xlm-roberta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Evaluating model: distilbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Evaluating model: mbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Results for xlm-roberta: {'eval_loss': 1.445087194442749, 'eval_model_preparation_time': 0.0075, 'eval_accuracy': 0.7983134223471539, 'eval_precision': 0.11460855528652139, 'eval_recall': 0.1419822522184727, 'eval_f1': 0.12683525930888181, 'eval_runtime': 5.2607, 'eval_samples_per_second': 2.471, 'eval_steps_per_second': 0.19}\nResults for distilbert: {'eval_loss': 0.6650081872940063, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.8673036093418259, 'eval_precision': 0.12390051562026085, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.13270527085194508, 'eval_runtime': 1.772, 'eval_samples_per_second': 7.336, 'eval_steps_per_second': 0.564}\nResults for mbert: {'eval_loss': 0.6373586058616638, 'eval_model_preparation_time': 0.0061, 'eval_accuracy': 0.8425806451612903, 'eval_precision': 0.12036866359447004, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.13065226090436174, 'eval_runtime': 2.7103, 'eval_samples_per_second': 4.796, 'eval_steps_per_second': 0.369}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}