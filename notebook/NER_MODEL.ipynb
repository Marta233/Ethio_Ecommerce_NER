{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9509572,"sourceType":"datasetVersion","datasetId":5788325},{"sourceType":"datasetVersion","sourceId":9518763,"datasetId":5795272}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef load_conll_format(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = []\n        sentence = []\n        labels = []\n\n        for line in f:\n            line = line.strip()\n            if line == \"\":\n                if sentence:  # Only append if the sentence list is not empty\n                    data.append((sentence, labels))\n                    sentence = []\n                    labels = []\n            else:\n                parts = line.split()\n                if len(parts) == 2:  # Ensure there are exactly two parts\n                    token, label = parts\n                    sentence.append(token)\n                    labels.append(label)\n                else:\n                    print(f\"Skipping line: {line}\")  # Optional: print to debug which lines are problematic\n\n        if sentence:  # Append the last sentence if the file doesn't end with a newline\n            data.append((sentence, labels))\n\n    return pd.DataFrame(data, columns=['tokens', 'labels'])\n\ndf = load_conll_format(\"/kaggle/input/newenewnewn/dataset.txt\")","metadata":{"id":"TUo0s-HgYtw1","execution":{"iopub.status.busy":"2024-09-30T19:09:36.983008Z","iopub.execute_input":"2024-09-30T19:09:36.983298Z","iopub.status.idle":"2024-09-30T19:09:41.267877Z","shell.execute_reply.started":"2024-09-30T19:09:36.983264Z","shell.execute_reply":"2024-09-30T19:09:41.266596Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Skipping line: Ô∏è\nSkipping line: Ô∏è\nSkipping line: Ô∏è\nSkipping line: Ô∏è\nSkipping line: Ô∏è\nSkipping line: Ô∏è\nSkipping line: Ô∏è\nSkipping line: Ô∏è\nSkipping line: Ô∏è\n","output_type":"stream"}]},{"cell_type":"code","source":"df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OvJ5qb7pYxN6","outputId":"3019854e-c3c3-473c-9d4b-00e6bf99bfb0","execution":{"iopub.status.busy":"2024-09-30T19:10:08.159324Z","iopub.execute_input":"2024-09-30T19:10:08.159812Z","iopub.status.idle":"2024-09-30T19:10:08.204137Z","shell.execute_reply.started":"2024-09-30T19:10:08.159774Z","shell.execute_reply":"2024-09-30T19:10:08.203238Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                               tokens  \\\n0   [2, ·àä·âµ·à≠, ·çî·à≠·àô·àµ, ·àà·â§·âµ, ·àà·â¢·àÆ, ·àà·àÜ·â¥·àç, ·ä†·åà·àç·åç·àé·âµ, ·àò·ãã·àç, ·ã®·àö...   \n1   [·â¢·àã, ·ã®·àµ·åã, ·ã®·ä†·å•·äï·âµ, 1000, ·â•·à≠, ·àà·àõ·ãò·ãù, ·ãç·àµ·äï, ·çç·à¨, ·äê·ãç, ...   \n2   [·ãã·àµ·âµ·äì, ·âÖ·äì·àΩ, ·ä†·ãµ·à´·àª, ·âÅ·å•·à≠, 1, ·àò·åà·äì·äõ, ·ãò·çç·àò·àΩ, ·åç·à´·äï·ãµ, ·àû·àç...   \n3   [2, ·ä†·äï·ãµ, ·â•·à©·àΩ, ·ä•·äì, ·ä†·äï·ãµ, ·àµ·çì·âπ·àã, ·ãã·åã, 300, ·â•·à≠, ·àà·àõ·ãò·ãù...   \n4   [2, ·ä†·äï·ãµ, ·â•·à©·àΩ, ·ä•·äì, ·ä†·äï·ãµ, ·àµ·çì·âπ·àã, ·ãã·åã, 300, ·â•·à≠, ·ãç·àµ·äï,...   \n..                                                ...   \n56  [·ãã·åã, 1000, ·â•·à≠, ·àà·àõ·ãò·ãù, ·ãç·àµ·äï, ·çç·à¨, ·äê·ãç, ·ã®·âÄ·à®·ãç, Ô∏è·å•·à´·âµ, ...   \n57  [·àà·â§·â∂, ·àà·àµ·å¶·â≥, ·â†·å•·à´·âµ, ·â†·âÖ·äì·àΩ, ·ãç·àµ·äï, ·çç·à¨, ·ã®·âÄ·à©, ·ãï·âÉ·ãà·âΩ, ·ä†·àÅ...   \n58  [·àà·àç·åÜ, ·àà·àµ·å¶·â≥, ·â†·å•·à´·âµ, ·â†·âÖ·äì·àΩ, ·ãç·àµ·äï, ·çç·à¨, ·ã®·âÄ·à©, ·ãï·âÉ·ãà·âΩ, ·ä†·àÅ...   \n59  [4, 1, 400, 10000, ·â•·à≠, ·ä®, ·äê·çÉ, ·àõ·ãµ·à®·àµ, ·åã·à≠, ·àà·àõ·ãò·ãù, ...   \n60  [4, 1, 400, 10000, ·â•·à≠, ·ä®, ·äê·çÉ, ·àõ·ãµ·à®·àµ, ·åã·à≠, ·àà·àõ·ãò·ãù, ...   \n\n                                               labels  \n0   [O, O, O, O, O, O, O, O, O, O, B-PRODUCT, O, O...  \n1   [B-PRODUCT, O, O, B-PRICE, I-PRICE, O, O, O, O...  \n2   [O, O, O, O, O, B-LOC, I-LOC, I-LOC, I-LOC, I-...  \n3   [O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE, I-...  \n4   [O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE, I-...  \n..                                                ...  \n56  [B-PRICE, I-PRICE, I-PRICE, O, O, O, O, O, O, ...  \n57  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n58  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n59  [O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...  \n60  [O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...  \n\n[61 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokens</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[2, ·àä·âµ·à≠, ·çî·à≠·àô·àµ, ·àà·â§·âµ, ·àà·â¢·àÆ, ·àà·àÜ·â¥·àç, ·ä†·åà·àç·åç·àé·âµ, ·àò·ãã·àç, ·ã®·àö...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, B-PRODUCT, O, O...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[·â¢·àã, ·ã®·àµ·åã, ·ã®·ä†·å•·äï·âµ, 1000, ·â•·à≠, ·àà·àõ·ãò·ãù, ·ãç·àµ·äï, ·çç·à¨, ·äê·ãç, ...</td>\n      <td>[B-PRODUCT, O, O, B-PRICE, I-PRICE, O, O, O, O...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[·ãã·àµ·âµ·äì, ·âÖ·äì·àΩ, ·ä†·ãµ·à´·àª, ·âÅ·å•·à≠, 1, ·àò·åà·äì·äõ, ·ãò·çç·àò·àΩ, ·åç·à´·äï·ãµ, ·àû·àç...</td>\n      <td>[O, O, O, O, O, B-LOC, I-LOC, I-LOC, I-LOC, I-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[2, ·ä†·äï·ãµ, ·â•·à©·àΩ, ·ä•·äì, ·ä†·äï·ãµ, ·àµ·çì·âπ·àã, ·ãã·åã, 300, ·â•·à≠, ·àà·àõ·ãò·ãù...</td>\n      <td>[O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE, I-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[2, ·ä†·äï·ãµ, ·â•·à©·àΩ, ·ä•·äì, ·ä†·äï·ãµ, ·àµ·çì·âπ·àã, ·ãã·åã, 300, ·â•·à≠, ·ãç·àµ·äï,...</td>\n      <td>[O, O, B-PRODUCT, O, O, B-PRODUCT, B-PRICE, I-...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>[·ãã·åã, 1000, ·â•·à≠, ·àà·àõ·ãò·ãù, ·ãç·àµ·äï, ·çç·à¨, ·äê·ãç, ·ã®·âÄ·à®·ãç, Ô∏è·å•·à´·âµ, ...</td>\n      <td>[B-PRICE, I-PRICE, I-PRICE, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>[·àà·â§·â∂, ·àà·àµ·å¶·â≥, ·â†·å•·à´·âµ, ·â†·âÖ·äì·àΩ, ·ãç·àµ·äï, ·çç·à¨, ·ã®·âÄ·à©, ·ãï·âÉ·ãà·âΩ, ·ä†·àÅ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>[·àà·àç·åÜ, ·àà·àµ·å¶·â≥, ·â†·å•·à´·âµ, ·â†·âÖ·äì·àΩ, ·ãç·àµ·äï, ·çç·à¨, ·ã®·âÄ·à©, ·ãï·âÉ·ãà·âΩ, ·ä†·àÅ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>[4, 1, 400, 10000, ·â•·à≠, ·ä®, ·äê·çÉ, ·àõ·ãµ·à®·àµ, ·åã·à≠, ·àà·àõ·ãò·ãù, ...</td>\n      <td>[O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>[4, 1, 400, 10000, ·â•·à≠, ·ä®, ·äê·çÉ, ·àõ·ãµ·à®·àµ, ·åã·à≠, ·àà·àõ·ãò·ãù, ...</td>\n      <td>[O, O, O, B-PRICE, I-PRICE, O, O, O, O, O, O, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>61 rows √ó 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Define label mappings\nlabel_to_id = {\n    \"O\": 0,\n    \"B-PRICE\": 1,\n    \"I-PRICE\": 2,\n    \"B-LOC\": 3,\n    \"I-LOC\": 4,\n    \"B-PRODUCT\": 5,\n    \"I-PRODUCT\": 6,\n    # Add other labels as needed\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:11:07.653485Z","iopub.execute_input":"2024-09-30T19:11:07.653861Z","iopub.status.idle":"2024-09-30T19:11:07.658607Z","shell.execute_reply.started":"2024-09-30T19:11:07.653827Z","shell.execute_reply":"2024-09-30T19:11:07.657702Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# Model names for fine-tuning\nmodel_names = {\n    \"xlm-roberta\": \"xlm-roberta-base\",\n    \"distilbert\": \"distilbert-base-multilingual-cased\",\n    \"mbert\": \"bert-base-multilingual-cased\"\n}\n\ntokenizers = {}\nmodels = {}\n\n# Load tokenizers and models\nfor model_key, model_name in model_names.items():\n    tokenizers[model_key] = AutoTokenizer.from_pretrained(model_name)\n    models[model_key] = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_to_id))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":217},"id":"KHOtWLpFOcqS","outputId":"83ae77da-801a-4bf6-88ee-9fbe9dc72021","execution":{"iopub.status.busy":"2024-09-30T19:11:12.574755Z","iopub.execute_input":"2024-09-30T19:11:12.575860Z","iopub.status.idle":"2024-09-30T19:11:32.394043Z","shell.execute_reply.started":"2024-09-30T19:11:12.575815Z","shell.execute_reply":"2024-09-30T19:11:32.393278Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"143c402b6b544f25840b97f14c11a2ca"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e039399e3104325840ec1b5ea59af8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854c2bd681a34783bdd58eaf90fc1bd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdf8ad1d362d4621835c6af3cf2e8e5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d71dbf3af1c74b83abd1e8e5e0f60b91"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f78caafc9642c9bc9e23c7b175beaf"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b597b5fcf734509ab3cc669d8ad608b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7941bf291ba64721a5e1a07df2091325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1308f4a1690d4b9dac177ef5e78a075d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8020dbadaf43e3b937447e87db9b69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90539117be954ed3a517d2a11f9538d1"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Define function to tokenize and align labels\ndef tokenize_and_align_labels(dataframe, tokenizer):\n    tokenized_inputs = tokenizer(\n        list(dataframe['tokens']),\n        truncation=True,\n        is_split_into_words=True,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n\n    labels = []\n    for i, label in enumerate(dataframe['labels']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = [-100] * len(tokenized_inputs['input_ids'][i])  # Use -100 for padding tokens\n        \n        for word_index in range(len(word_ids)):\n            if word_ids[word_index] is not None:  # Check if it's a valid word token\n                current_label = label[word_ids[word_index]]\n                label_ids[word_index] = label_to_id[current_label]\n        \n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Tokenize data for each model\ntokenized_data = {}\nfor model_key, tokenizer in tokenizers.items():\n    tokenized_data[model_key] = tokenize_and_align_labels(df, tokenizer)\n","metadata":{"id":"yfvLXCVsQ9TC","execution":{"iopub.status.busy":"2024-09-30T19:11:51.002596Z","iopub.execute_input":"2024-09-30T19:11:51.003169Z","iopub.status.idle":"2024-09-30T19:11:51.858683Z","shell.execute_reply.started":"2024-09-30T19:11:51.003128Z","shell.execute_reply":"2024-09-30T19:11:51.857884Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Convert the tokenized data into Hugging Face Datasets for training and validation\ndatasets = {}\nfor model_key, data in tokenized_data.items():\n    dataset = Dataset.from_dict({\n        'input_ids': data['input_ids'],\n        'attention_mask': data['attention_mask'],\n        'labels': data['labels']\n    })\n\n    # Split the dataset into training and validation sets (80/20 split)\n    datasets[model_key] = dataset.train_test_split(test_size=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:12:08.004996Z","iopub.execute_input":"2024-09-30T19:12:08.005932Z","iopub.status.idle":"2024-09-30T19:12:08.143685Z","shell.execute_reply.started":"2024-09-30T19:12:08.005889Z","shell.execute_reply":"2024-09-30T19:12:08.142930Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n    logging_steps=10,  # Log after every 10 steps\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",  # Directory to save logs\n    logging_first_step=True,  # Log the first step as well\n)\n\n# Fine-tune each model and store the trainers\ntrainers = {}\nfor model_key, model in models.items():\n    trainers[model_key] = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=datasets[model_key]['train'],\n        eval_dataset=datasets[model_key]['test']\n    )\n\n    # Train the model\n    print(f\"Training model: {model_key}\")\n    trainers[model_key].train()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:15:06.703810Z","iopub.execute_input":"2024-09-30T19:15:06.704513Z","iopub.status.idle":"2024-09-30T19:15:47.510126Z","shell.execute_reply.started":"2024-09-30T19:15:06.704457Z","shell.execute_reply":"2024-09-30T19:15:47.509292Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training model: xlm-roberta\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.829400</td>\n      <td>0.742567</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.829400</td>\n      <td>0.618416</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.829400</td>\n      <td>0.610165</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Training model: distilbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.581000</td>\n      <td>0.513163</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.581000</td>\n      <td>0.483254</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.581000</td>\n      <td>0.468558</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Training model: mbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.551700</td>\n      <td>0.626722</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.551700</td>\n      <td>0.586671</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.551700</td>\n      <td>0.576116</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the models on the validation dataset\nresults = {}\nfor model_key, trainer in trainers.items():\n    print(f\"Evaluating model: {model_key}\")\n    eval_result = trainer.evaluate()\n    results[model_key] = eval_result\n\n# Print the results\nfor model_key, result in results.items():\n    print(f\"Results for {model_key}: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:16:59.465135Z","iopub.execute_input":"2024-09-30T19:16:59.466042Z","iopub.status.idle":"2024-09-30T19:16:59.957864Z","shell.execute_reply.started":"2024-09-30T19:16:59.465997Z","shell.execute_reply":"2024-09-30T19:16:59.956901Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Evaluating model: xlm-roberta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluating model: distilbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluating model: mbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Results for xlm-roberta: {'eval_loss': 0.6101651191711426, 'eval_runtime': 0.2765, 'eval_samples_per_second': 47.019, 'eval_steps_per_second': 3.617, 'epoch': 3.0}\nResults for distilbert: {'eval_loss': 0.4685583710670471, 'eval_runtime': 0.0782, 'eval_samples_per_second': 166.301, 'eval_steps_per_second': 12.792, 'epoch': 3.0}\nResults for mbert: {'eval_loss': 0.5761160254478455, 'eval_runtime': 0.1149, 'eval_samples_per_second': 113.14, 'eval_steps_per_second': 8.703, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    # Flatten and filter out padding tokens (-100)\n    true_labels = []\n    predicted_labels = []\n    \n    for i in range(len(labels)):\n        true_labels.extend([label for label, pred_label in zip(labels[i], preds[i]) if label != -100])\n        predicted_labels.extend([pred_label for label, pred_label in zip(labels[i], preds[i]) if label != -100])\n    \n    # Compute metrics\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    precision = precision_score(true_labels, predicted_labels, average=\"macro\")\n    recall = recall_score(true_labels, predicted_labels, average=\"macro\")\n    f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:18:21.107445Z","iopub.execute_input":"2024-09-30T19:18:21.108173Z","iopub.status.idle":"2024-09-30T19:18:21.118070Z","shell.execute_reply.started":"2024-09-30T19:18:21.108128Z","shell.execute_reply":"2024-09-30T19:18:21.117273Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainers = {}\nfor model_key, model in models.items():\n    trainers[model_key] = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=datasets[model_key]['train'],\n        eval_dataset=datasets[model_key]['test'],\n        compute_metrics=compute_metrics  # Use the updated compute_metrics function\n    )\n\n# Evaluate the models\nresults = {}\nfor model_key, trainer in trainers.items():\n    print(f\"Evaluating model: {model_key}\")\n    eval_result = trainer.evaluate()\n    results[model_key] = eval_result\n\n# Print results\nfor model_key, result in results.items():\n    print(f\"Results for {model_key}: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:18:21.119869Z","iopub.execute_input":"2024-09-30T19:18:21.120511Z","iopub.status.idle":"2024-09-30T19:18:21.763199Z","shell.execute_reply.started":"2024-09-30T19:18:21.120466Z","shell.execute_reply":"2024-09-30T19:18:21.762004Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Evaluating model: xlm-roberta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluating model: distilbert\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Evaluating model: mbert\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Results for xlm-roberta: {'eval_loss': 0.6101651191711426, 'eval_accuracy': 0.8245496997998666, 'eval_precision': 0.1177928142571238, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.1291198746408984, 'eval_runtime': 0.3262, 'eval_samples_per_second': 39.857, 'eval_steps_per_second': 3.066}\nResults for distilbert: {'eval_loss': 0.4685583710670471, 'eval_accuracy': 0.8668941979522184, 'eval_precision': 0.12384202827888835, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.13267171585270304, 'eval_runtime': 0.0913, 'eval_samples_per_second': 142.393, 'eval_steps_per_second': 10.953}\nResults for mbert: {'eval_loss': 0.5761160254478455, 'eval_accuracy': 0.8474114441416893, 'eval_precision': 0.12105877773452704, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.13105773282764432, 'eval_runtime': 0.1274, 'eval_samples_per_second': 102.032, 'eval_steps_per_second': 7.849}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom transformers import Trainer, TrainingArguments\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n\n    # Flatten and filter out padding tokens (-100)\n    true_labels = []\n    predicted_labels = []\n\n    for i in range(len(labels)):\n        true_labels.extend([label for label, pred_label in zip(labels[i], preds[i]) if label != -100])\n        predicted_labels.extend([pred_label for label, pred_label in zip(labels[i], preds[i]) if label != -100])\n\n    # Compute metrics\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    precision = precision_score(true_labels, predicted_labels, average=\"macro\")\n    recall = recall_score(true_labels, predicted_labels, average=\"macro\")\n    f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n# Assuming models and datasets are defined elsewhere\n# models = {...}\n# datasets = {...}\n\ntrainers = {}\n\n# Define TrainingArguments for Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",  # Directory to save checkpoints and results\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    logging_dir=\"./logs\",  # Directory for storing logs\n    logging_steps=10,  # Number of steps between logging updates\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,  # Example: Train for 3 epochs\n    save_steps=100,  # Save model checkpoints every 100 steps\n    logging_first_step=True,\n)\n\n# Create trainers for each model\nfor model_key, model in models.items():\n    trainers[model_key] = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=datasets[model_key]['train'],\n        eval_dataset=datasets[model_key]['test'],\n        compute_metrics=compute_metrics,\n    )\n\n# Train and evaluate each model\nresults = {}\nfor model_key, trainer in trainers.items():\n    print(f\"Training and evaluating model: {model_key}\")\n    trainer.train()\n    eval_result = trainer.evaluate()\n\n    results[model_key] = eval_result\n    print(f\"Results for {model_key}: {eval_result}\")\n\n# Optionally, you can access the evaluation results per epoch from the Trainer object directly.\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:27:33.178088Z","iopub.execute_input":"2024-09-30T19:27:33.178875Z","iopub.status.idle":"2024-09-30T19:28:14.827245Z","shell.execute_reply.started":"2024-09-30T19:27:33.178832Z","shell.execute_reply":"2024-09-30T19:28:14.826152Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Training and evaluating model: xlm-roberta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.138800</td>\n      <td>0.162264</td>\n      <td>0.973316</td>\n      <td>0.415035</td>\n      <td>0.424536</td>\n      <td>0.419589</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.138800</td>\n      <td>0.150800</td>\n      <td>0.973316</td>\n      <td>0.413809</td>\n      <td>0.426370</td>\n      <td>0.419883</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.138800</td>\n      <td>0.145427</td>\n      <td>0.969980</td>\n      <td>0.416641</td>\n      <td>0.422312</td>\n      <td>0.419446</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Results for xlm-roberta: {'eval_loss': 0.14542677998542786, 'eval_accuracy': 0.9699799866577719, 'eval_precision': 0.4166411355243466, 'eval_recall': 0.42231170932627243, 'eval_f1': 0.4194464951177891, 'eval_runtime': 0.234, 'eval_samples_per_second': 55.552, 'eval_steps_per_second': 4.273, 'epoch': 3.0}\nTraining and evaluating model: distilbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:12, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.171300</td>\n      <td>0.243769</td>\n      <td>0.932878</td>\n      <td>0.548666</td>\n      <td>0.455753</td>\n      <td>0.423656</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.171300</td>\n      <td>0.136208</td>\n      <td>0.961320</td>\n      <td>0.499090</td>\n      <td>0.535590</td>\n      <td>0.516065</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.171300</td>\n      <td>0.136929</td>\n      <td>0.965870</td>\n      <td>0.642508</td>\n      <td>0.588259</td>\n      <td>0.573261</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Results for distilbert: {'eval_loss': 0.13692906498908997, 'eval_accuracy': 0.9658703071672355, 'eval_precision': 0.6425079300870166, 'eval_recall': 0.5882592800899887, 'eval_f1': 0.5732605202151494, 'eval_runtime': 0.1093, 'eval_samples_per_second': 118.963, 'eval_steps_per_second': 9.151, 'epoch': 3.0}\nTraining and evaluating model: mbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9/9 00:10, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.207800</td>\n      <td>0.393601</td>\n      <td>0.871935</td>\n      <td>0.395734</td>\n      <td>0.234014</td>\n      <td>0.259467</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.207800</td>\n      <td>0.255662</td>\n      <td>0.934605</td>\n      <td>0.500282</td>\n      <td>0.460569</td>\n      <td>0.456918</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.207800</td>\n      <td>0.245124</td>\n      <td>0.933243</td>\n      <td>0.488957</td>\n      <td>0.455696</td>\n      <td>0.448963</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Results for mbert: {'eval_loss': 0.24512434005737305, 'eval_accuracy': 0.9332425068119891, 'eval_precision': 0.48895739740022653, 'eval_recall': 0.4556964586477678, 'eval_f1': 0.4489632563593071, 'eval_runtime': 0.1478, 'eval_samples_per_second': 87.964, 'eval_steps_per_second': 6.766, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import shap\nimport torch\n\n# Choose a sample text from your validation dataset for SHAP analysis\nsample_text = [\"·ã≠·àÖ ·àù·à≠·âµ ·â†·ä†·ã≤·àµ ·ä†·â†·â£ ·ä•·åÖ·åç ·â†·à≠·ä´·â≥ ·åà·äï·ãò·â• ·ã≠·äñ·à´·àç·ç¢\"]\n\n# Select a model for SHAP analysis (e.g., \"xlm-roberta\")\nmodel_key = 'xlm-roberta'\nmodel = models[model_key]\ntokenizer = tokenizers[model_key]\n\n# Tokenize the sample text\ntokenized_sample = tokenizer(\n    sample_text,\n    truncation=True,\n    padding=True,\n    return_tensors=\"pt\"\n)\n\n# Define prediction function for SHAP\ndef prediction_function(inputs):\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n    return predictions.cpu().numpy()\n\n# Convert tokenized input to SHAP-compatible format\nexplainer = shap.Explainer(prediction_function, tokenizer)\nshap_values = explainer(tokenized_sample)\n\n# Visualize the SHAP values\nshap.plots.text(shap_values[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:33:51.861325Z","iopub.execute_input":"2024-09-30T19:33:51.861757Z","iopub.status.idle":"2024-09-30T19:33:55.356094Z","shell.execute_reply.started":"2024-09-30T19:33:51.861718Z","shell.execute_reply":"2024-09-30T19:33:55.354511Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Convert tokenized input to SHAP-compatible format\u001b[39;00m\n\u001b[1;32m     29\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(prediction_function, tokenizer)\n\u001b[0;32m---> 30\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Visualize the SHAP values\u001b[39;00m\n\u001b[1;32m     33\u001b[0m shap\u001b[38;5;241m.\u001b[39mplots\u001b[38;5;241m.\u001b[39mtext(shap_values[\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/_partition.py:129\u001b[0m, in \u001b[0;36mPartitionExplainer.__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, fixed_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    126\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/_explainer.py:267\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 267\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    272\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/_partition.py:154\u001b[0m, in \u001b[0;36mPartitionExplainer.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# if not fixed background or no base value assigned then compute base value for a row\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_base_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_background\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_base_value \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm00\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# the zero index param tells the masked model what the baseline is\u001b[39;00m\n\u001b[1;32m    155\u001b[0m f11 \u001b[38;5;241m=\u001b[39m fm(\u001b[38;5;241m~\u001b[39mm00\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker\u001b[38;5;241m.\u001b[39mclustering):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/utils/_masked_model.py:69\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_masking_call(full_masks, zero_index\u001b[38;5;241m=\u001b[39mzero_index, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/utils/_masked_model.py:146\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    143\u001b[0m         all_masked_inputs[i]\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[1;32m    145\u001b[0m joined_masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([np\u001b[38;5;241m.\u001b[39mconcatenate(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_masked_inputs])\n\u001b[0;32m--> 146\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjoined_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[1;32m    148\u001b[0m all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/models/_model.py:28\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 28\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m safe_isinstance(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m is_tensor \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(out)\n","Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mprediction_function\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_function\u001b[39m(inputs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     24\u001b[0m         logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     25\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: XLMRobertaForTokenClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n) argument after ** must be a mapping, not numpy.ndarray"],"ename":"TypeError","evalue":"XLMRobertaForTokenClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n) argument after ** must be a mapping, not numpy.ndarray","output_type":"error"}]},{"cell_type":"code","source":"from lime.lime_text import LimeTextExplainer\n\n# Create an instance of LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=[\"O\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\", \"B-PRODUCT\", \"I-PRODUCT\"])\n\n# Define the prediction function for LIME\ndef lime_prediction_function(texts):\n    # Tokenize input texts\n    tokenized_inputs = tokenizer(\n        texts,\n        truncation=True,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n\n    with torch.no_grad():\n        # Predict with model\n        outputs = model(**tokenized_inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n    \n    # Convert predictions to the format expected by LIME\n    return predictions.cpu().numpy()\n\n# Sample text for LIME analysis\nsample_text = \"·ã≠·àÖ ·àù·à≠·âµ ·â†·ä†·ã≤·àµ ·ä†·â†·â£ ·ä•·åÖ·åç ·â†·à≠·ä´·â≥ ·åà·äï·ãò·â• ·ã≠·äñ·à´·àç·ç¢\"\n\n# Generate LIME explanation\nlime_explanation = explainer.explain_instance(\n    sample_text,\n    classifier_fn=lime_prediction_function,\n    num_features=10  # Number of features to show\n)\n\n# Display the LIME explanation in a notebook (for Jupyter/Colab environments)\nlime_explanation.show_in_notebook(text=True)\n\n# To view LIME explanation in other environments, you can export it as HTML\n# lime_explanation.save_to_file('lime_explanation.html')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:34:36.297968Z","iopub.execute_input":"2024-09-30T19:34:36.298738Z","iopub.status.idle":"2024-09-30T19:34:37.980217Z","shell.execute_reply.started":"2024-09-30T19:34:36.298697Z","shell.execute_reply":"2024-09-30T19:34:37.978479Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m·ã≠·àÖ ·àù·à≠·âµ ·â†·ä†·ã≤·àµ ·ä†·â†·â£ ·ä•·åÖ·åç ·â†·à≠·ä´·â≥ ·åà·äï·ãò·â• ·ã≠·äñ·à´·àç·ç¢\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Generate LIME explanation\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m lime_explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlime_prediction_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of features to show\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Display the LIME explanation in a notebook (for Jupyter/Colab environments)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m lime_explanation\u001b[38;5;241m.\u001b[39mshow_in_notebook(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lime/lime_text.py:413\u001b[0m, in \u001b[0;36mLimeTextExplainer.explain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    406\u001b[0m indexed_string \u001b[38;5;241m=\u001b[39m (IndexedCharacters(\n\u001b[1;32m    407\u001b[0m     text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow, mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string)\n\u001b[1;32m    408\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_level \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    409\u001b[0m                   IndexedString(text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow,\n\u001b[1;32m    410\u001b[0m                                 split_expression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_expression,\n\u001b[1;32m    411\u001b[0m                                 mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string))\n\u001b[1;32m    412\u001b[0m domain_mapper \u001b[38;5;241m=\u001b[39m TextDomainMapper(indexed_string)\n\u001b[0;32m--> 413\u001b[0m data, yss, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__data_labels_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexed_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(yss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lime/lime_text.py:482\u001b[0m, in \u001b[0;36mLimeTextExplainer.__data_labels_distances\u001b[0;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    480\u001b[0m     data[i, inactive] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    481\u001b[0m     inverse_data\u001b[38;5;241m.\u001b[39mappend(indexed_string\u001b[38;5;241m.\u001b[39minverse_removing(inactive))\n\u001b[0;32m--> 482\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverse_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m distances \u001b[38;5;241m=\u001b[39m distance_fn(sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix(data))\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, labels, distances\n","Cell \u001b[0;32mIn[18], line 18\u001b[0m, in \u001b[0;36mlime_prediction_function\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      9\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     10\u001b[0m     texts,\n\u001b[1;32m     11\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Predict with model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     20\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1401\u001b[0m, in \u001b[0;36mXLMRobertaForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1401\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1415\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:827\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    825\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 827\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    835\u001b[0m     embedding_output,\n\u001b[1;32m    836\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    844\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:116\u001b[0m, in \u001b[0;36mXLMRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    113\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    119\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}